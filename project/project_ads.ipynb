{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61847f62",
   "metadata": {},
   "source": [
    "# ADS Project Gruppe 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8df3dd",
   "metadata": {},
   "source": [
    "## Calling WebApi from Google for Restaurants near Zurich and Save the Data in a CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2f03b7-735c-4a16-8469-cb485603187b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import json\n",
    "import psycopg2\n",
    "import time\n",
    "import folium\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "final_data = []\n",
    "\n",
    "# Parameters\n",
    "coordinates = ['47.36667,8.55']  \n",
    "keywords = ['restaurant']\n",
    "radius = '10000'\n",
    "api_key = 'AIzaSyBCHz76l1D5ZJ-R9K7tawuL9aOR4Yz5fco'\n",
    "\n",
    "for coordinate in coordinates:\n",
    "    for keyword in keywords:\n",
    "        url = 'https://maps.googleapis.com/maps/api/place/nearbysearch/json?location=' + coordinate + '&radius=' + str(radius) + '&keyword=' + str(keyword) + '&key=' + str(api_key)\n",
    "        while True:\n",
    "            print(url)\n",
    "            response = requests.get(url)\n",
    "            jj = json.loads(response.text)\n",
    "            results = jj['results']\n",
    "            for result in results:\n",
    "                name = result['name']\n",
    "                place_id = result['place_id']\n",
    "                lat = result['geometry']['location']['lat']\n",
    "                lng = result['geometry']['location']['lng']\n",
    "                rating = result.get('rating', 'N/A')\n",
    "                user_ratings_total = result['user_ratings_total']\n",
    "                types = result['types']\n",
    "                vicinity = result['vicinity']\n",
    "                price_level = result.get('price_level',0)\n",
    "                data = [name, place_id, lat, lng, rating, user_ratings_total, types, price_level, vicinity]\n",
    "                final_data.append(data)\n",
    "            time.sleep(5)\n",
    "            \n",
    "            if 'next_page_token' not in jj:\n",
    "                break\n",
    "            else:\n",
    "                next_page_token = jj['next_page_token']\n",
    "                time.sleep(5)\n",
    "                url = 'https://maps.googleapis.com/maps/api/place/nearbysearch/json?key=' + str(api_key) + '&pagetoken=' + str(next_page_token)\n",
    "\n",
    "labels = ['Place Name', 'Place ID', 'Latitude', 'Longitude', 'Rating','User Ratings Total','Types','Price Level', 'Vicinity']\n",
    "export_dataframe_1_medium = pd.DataFrame.from_records(final_data, columns=labels)\n",
    "export_dataframe_1_medium.to_csv('export_restaurants_near_zurich.csv')\n",
    "print(\"finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdfd2ab-70f4-45bf-826c-d69540f10e17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SSL_VERIFY = False\n",
    "# evtl. SSL_VERIFY auf False setzen wenn die Verbindung zu https://www.zuerich.com nicht klappt (z.B. wegen Proxy)\n",
    "# Um die SSL Verifikation auszustellen, bitte die nächste Zeile einkommentieren (\"#\" entfernen)\n",
    "# SSL_VERIFY = False\n",
    "if not SSL_VERIFY:\n",
    "    import urllib3\n",
    "    urllib3.disable_warnings()\n",
    "    \n",
    "def get_de(field):\n",
    "    try:\n",
    "        return field['de']\n",
    "    except (KeyError, TypeError):\n",
    "        try:\n",
    "            return field['en']\n",
    "        except (KeyError, TypeError):\n",
    "            return field\n",
    "\n",
    "\n",
    "\n",
    "headers = {'Accept': 'application/json'}\n",
    "r = requests.get('https://www.zuerich.com/en/api/v2/data?id=101', headers=headers, verify=SSL_VERIFY)\n",
    "\n",
    "\n",
    "data = r.json()\n",
    "\n",
    "de_data = [{k: get_de(v) for (k,v) in f.items()} for f in data]\n",
    "de_data\n",
    "\n",
    "\n",
    "df = pd.DataFrame(de_data)\n",
    "df.to_csv('export_restaurants_near_zurich_STADTzuerich.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf2d26a-d484-447c-96e8-044dee5d9d58",
   "metadata": {},
   "source": [
    "## Web Scrapping Trip Advisor Review Restaurant Zeughauskeller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f54762-c264-4b06-9d87-29c4335f9e4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import csv\n",
    "\n",
    "\n",
    "base_url = 'https://www.tripadvisor.ch/Restaurant_Review-g188113-d697919-Reviews-Restaurant_Zeughauskeller-Zurich.html'\n",
    "\n",
    "# Erstelle eine Session und akzeptiere Cookies\n",
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123 Safari/537.36',\n",
    "    'Accept-Language': 'en-US,en;q=0.9',\n",
    "})\n",
    "\n",
    "# Liste für die Bewertungen\n",
    "reviews = []\n",
    "\n",
    "# Starte die Schleife für die Abfragen\n",
    "for i in range(0, 100):  # Hier kannst du die Anzahl der Abfragen anpassen (in diesem Fall 4)\n",
    "    if i == 0:\n",
    "        url = base_url\n",
    "    else:\n",
    "        offset = i * 10\n",
    "        url = base_url.replace('Reviews-', 'Reviews-or{}-'.format(offset))\n",
    "\n",
    "    # Lade den HTML-Code der Webseite herunter\n",
    "    response = session.get(url)\n",
    "    html_content = response.text\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    review_elements = soup.find_all('div', class_='reviewSelector')\n",
    "\n",
    "    for review_element in review_elements:\n",
    "        # Restaurant\n",
    "        restaurant = 'Zeughauskeller'\n",
    "\n",
    "        # Review\n",
    "        review_text_element = review_element.find('p', class_='partial_entry')\n",
    "        if review_text_element:\n",
    "            review_text = re.sub('<.*?>', '', str(review_text_element))\n",
    "            review_text = review_text.strip()\n",
    "        else:\n",
    "            review_text = ''\n",
    "\n",
    "        # Points\n",
    "        points_element = review_element.find('span', class_=re.compile('bubble_([0-9]+)'))\n",
    "        if points_element:\n",
    "            points_class = points_element.get('class')[1]\n",
    "            points = int(re.search(r'\\d+', points_class).group())/10\n",
    "        else:\n",
    "            points = None\n",
    "            \n",
    "        sentiment = 1 if points > 2 else 0\n",
    "\n",
    "        reviews.append({'Restaurant': restaurant, 'Review': review_text, 'Points': points, 'Sentiment':sentiment})\n",
    "\n",
    "# Schreibe die Bewertungen in eine CSV-Datei\n",
    "filename = 'tripadvisor_reviews.csv'\n",
    "\n",
    "with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=['Restaurant', 'Review', 'Points', 'Sentiment'])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(reviews)\n",
    "\n",
    "print('Die Bewertungen wurden in die Datei', filename, 'geschrieben.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc2ee8d",
   "metadata": {},
   "source": [
    "## Create database connection, read data and write to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a73766-07de-4d4a-941c-fdc2616c92ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\"host=db dbname=postgres user=admin password=secret\")\n",
    "\n",
    "df = pd.read_csv('export_restaurants_near_zurich.csv', sep=',')\n",
    "df.head(5)\n",
    "\n",
    "engine = create_engine('postgresql://admin:secret@db:5432/postgres')\n",
    "df.to_sql('restaurants_table', engine, if_exists='replace')\n",
    "\n",
    "df_sub = pd.read_sql_query('''SELECT\n",
    "                             \"Place Name\",\n",
    "                             \"Place ID\"\n",
    "                             FROM restaurants_table''', \n",
    "                          con=engine)\n",
    "df_sub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b80dc61",
   "metadata": {},
   "source": [
    "## Data preparation Restaurants Data near Zurich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077a187d-05be-4c30-a367-a4e1a19d4466",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Überprüfen Sie die Daten auf fehlende Werte:\n",
    "df.isnull().sum()\n",
    "\n",
    "#Entfernen Duplikate basierend auf der Place ID:\n",
    "df = df.drop_duplicates(subset='Place ID', keep='first')\n",
    "\n",
    "#Extrahieren den Ort und Distrikt aus der Spalte \"Vicinity\":\n",
    "df['Location'] = df['Vicinity'].str.split(',', expand=True)[0]\n",
    "df['District'] = df['Vicinity'].str.split(',', expand=True)[1]\n",
    "\n",
    "\n",
    "df = df[df['User Ratings Total'] >= 10]\n",
    "\n",
    "df.to_csv('cleaned_places_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a828fd6-ebe9-431d-b0ab-0ee97e96cf53",
   "metadata": {},
   "source": [
    "## Explorative Datenanalyse (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57b9583-89b5-4b58-b8a1-1349f21d086a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.info()\n",
    "\n",
    "df.head()\n",
    "\n",
    "df.describe()\n",
    "\n",
    "sns.histplot(df['Rating'], bins=10)\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Ratings')\n",
    "plt.show()\n",
    "\n",
    "top_10_places = df.nlargest(10, 'User Ratings Total')\n",
    "sns.barplot(x='User Ratings Total', y='Place Name', data=top_10_places, orient='h')\n",
    "plt.xlabel('User Ratings Total')\n",
    "plt.ylabel('Place Name')\n",
    "plt.title('Top 10 Places by User Ratings Total')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Zürich-Koordinaten für die Kartenzentrierung\n",
    "zurich_coords = [47.3769, 8.5417]\n",
    "\n",
    "# Erstellen der Karte\n",
    "map_zurich = folium.Map(location=zurich_coords, zoom_start=13)\n",
    "\n",
    "# Markieren der Orte auf der Karte\n",
    "for index, row in df.iterrows():\n",
    "    folium.Marker([row['Latitude'], row['Longitude']], popup=row['Place Name']).add_to(map_zurich)\n",
    "\n",
    "# Anzeigen der Karte\n",
    "map_zurich"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff525b1e",
   "metadata": {},
   "source": [
    "## Verwendung eines ML Frameworks/Library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1842db1-9d41-46bf-9e13-32d138497926",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Regression Model on basis of User Ratings Total, Rating to Price Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fbf4ef-7db0-4f8c-8cfe-b129380965ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Features auswählen\n",
    "features = df[['User Ratings Total', \"Rating\"]]\n",
    "label = df['Price Level']\n",
    "\n",
    "# Kategorische Features in numerische Features umwandeln (z.B. One-Hot-Encoding)\n",
    "features_encoded = pd.get_dummies(features)\n",
    "\n",
    "# Aufteilen der Daten in Trainings- und Testsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_encoded, label, test_size=0.2, random_state=42)\n",
    "\n",
    "# Skalieren der Features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Erstellen und Trainieren des Modells\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluieren des Modells auf dem Testset\n",
    "score = model.score(X_test_scaled, y_test)\n",
    "print('R-squared Score:', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374a3afe-c4a1-4284-aee4-6e115a50df5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Classification Price Level from the Rating and User Ratings Total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cae5b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Aufteilen der Daten in Features und Labels\n",
    "X = df[['Rating', 'User Ratings Total']]\n",
    "y = pd.get_dummies(df['Price Level'])\n",
    "\n",
    "# Aufteilen der Daten in Trainings- und Testsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Skalieren der Features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Erstellen des Modells\n",
    "model = Sequential()\n",
    "model.add(Dense(16, activation='relu', input_shape=(2,)))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "# Kompilieren und Trainieren des Modells\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train_scaled, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Vorhersagen auf dem Testset\n",
    "predicted_labels = model.predict(X_test_scaled)\n",
    "\n",
    "# Umkehren der One-Hot-Kodierung der Labels\n",
    "y_test_original = pd.get_dummies(df['Price Level']).columns[y_test.values.argmax(axis=1)]\n",
    "predicted_labels_original = pd.get_dummies(df['Price Level']).columns[predicted_labels.argmax(axis=1)]\n",
    "\n",
    "# Berechnung der Confusion Matrix\n",
    "cm = confusion_matrix(y_test_original, predicted_labels_original)\n",
    "print('Confusion Matrix:')\n",
    "print(cm)\n",
    "\n",
    "# Berechnung des Klassifikationsberichts\n",
    "classification_rep = classification_report(y_test_original, predicted_labels_original, zero_division=1)\n",
    "print('Classification Report:')\n",
    "print(classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d314601-8247-4d47-ad37-c8db232aa542",
   "metadata": {},
   "source": [
    "## Sentimentanalyse mit einem Convolutional Neural Network (CNN) zur Textanalyse für deutsche Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61887cf-970b-43e8-9636-aedf5238803d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
    "\n",
    "\n",
    "# Laden der Daten\n",
    "data = pd.read_csv('tripadvisor_reviews.csv')\n",
    "\n",
    "# Vorverarbeitung der Daten\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stop_words = set(stopwords.words('german'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Konvertierung in Kleinbuchstaben\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Entfernung von Satzzeichen und Sonderzeichen\n",
    "    text = re.sub(r\"[^a-zA-Z0-9äöüß]\", \" \", text)\n",
    "    \n",
    "    # Tokenisierung\n",
    "    tokens = word_tokenize(text, language='german')\n",
    "    \n",
    "    # Entfernung von Stoppwörtern\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    \n",
    "    # Stemming\n",
    "    stemmer = SnowballStemmer(\"german\")\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "    \n",
    "    # Zusammenfügen der verbleibenden Tokens zu einem Text\n",
    "    preprocessed_text = ' '.join(stemmed_tokens)\n",
    "    return preprocessed_text\n",
    "\n",
    "# Anwendung der Vorverarbeitungsfunktion auf die 'Review'-Spalte\n",
    "data['processed_text'] = data['Review'].apply(preprocess_text)\n",
    "\n",
    "# Aufteilung in Trainings- und Testdaten\n",
    "X = data['processed_text'].values\n",
    "y = data['Sentiment'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenisierung und Vektorisierung der Textdaten\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Padding der Sequenzen\n",
    "max_length = max(len(sequence) for sequence in X_train_sequences)\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_length, padding='post')\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Definition und Training des erweiterten CNN-Modells\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=max_length))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train_padded, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Auswertung des Modells\n",
    "loss, accuracy = model.evaluate(X_test_padded, y_test)\n",
    "print(\"Loss:\", loss)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Vorhersage auf neuen Texten\n",
    "new_texts = [\"Das Essen in diesem Restaurant hat mir sehr gut geschmeckt.\",\n",
    "             \"Der Service war schrecklich und das Essen war kalt. Katastrophe! es war miserabel, scheisse und schlecht. Ecklig, Kacke, nie wieder!\"]\n",
    "new_texts_sequences = tokenizer.texts_to_sequences(new_texts)\n",
    "new_texts_padded = pad_sequences(new_texts_sequences, maxlen=max_length, padding='post')\n",
    "predictions = model.predict(new_texts_padded)\n",
    "print(predictions)\n",
    "sentiments = ['positiv' if prediction >= 0.5 else 'negativ' for prediction in predictions]\n",
    "for text, sentiment in zip(new_texts, sentiments):\n",
    "    print(\"Text:\", text)\n",
    "    print(\"Sentiment:\", sentiment)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3aacc6b-4168-479d-8f15-314a4508869c",
   "metadata": {},
   "source": [
    "Modellierungshypothese: Das CNN-Modell kann sprachliche Muster und Wortzusammenhänge in den Bewertungen erkennen und daraus Rückschlüsse auf bestimmte Eigenschaften des Restaurants ziehen.\n",
    "Überprüfungsansatz: Untersuchen Sie die Gewichte der Conv1D-Schicht, um herauszufinden, welche Wortmuster oder n-gramme das Modell als besonders relevant für positive oder negative Bewertungen erachtet.\n",
    "\n",
    "Modellierungshypothese: Das CNN-Modell kann Schlüsselaspekte der Restauranterfahrung identifizieren, indem es die häufigsten Wörter oder Phrasen in den Bewertungen analysiert.\n",
    "Überprüfungsansatz: Extrahieren Sie die Top-N-Wörter oder n-gramme aus den Trainingsdaten und vergleichen Sie sie mit den Bewertungen, um festzustellen, ob die vom Modell als wichtig erachteten Wörter tatsächlich relevante Aspekte des Restaurants darstellen.\n",
    "\n",
    "Modellierungshypothese: Bestimmte Eigenschaften des Restaurants, wie die Preisklasse oder die Öffnungszeiten, korrelieren mit positiven oder negativen Bewertungen.\n",
    "Überprüfungsansatz: Fügen Sie zusätzliche Merkmale des Restaurants zu den Trainingsdaten hinzu und überprüfen Sie, ob das Modell diese Merkmale in Bezug auf die Bewertungen gewichtet. Führen Sie eine statistische Analyse durch, um die Korrelation zwischen den Merkmalen und den Bewertungen zu ermitteln."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "18632da57c1a416bf8be4aa27ba4ca7c1f66541805f18b0825a162dab4e44f29"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
